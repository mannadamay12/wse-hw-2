{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class QuerySet:\n",
    "    qid_to_query: Dict[str, str]\n",
    "    qid_to_embedding: Dict[str, np.ndarray]\n",
    "    qid_to_relevance: Dict[str, Dict[str, int]]\n",
    "\n",
    "def load_embeddings(file_path: str, id_key='id', embedding_key='embedding'):\n",
    "    \"\"\"Load embeddings from H5 file.\"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Print available keys in file for debugging\n",
    "        print(f\"Available keys in {file_path}: {list(f.keys())}\")\n",
    "        ids = np.array(f[id_key]).astype(str)\n",
    "        embeddings = np.array(f[embedding_key]).astype(np.float32)\n",
    "    print(f\"Loaded {len(ids)} embeddings from {file_path}\")\n",
    "    return ids, embeddings\n",
    "\n",
    "class MSMARCOData:\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.passages = None\n",
    "        self.passage_embeddings = None\n",
    "        self.eval_queries = None\n",
    "        self.dev_queries = None\n",
    "    \n",
    "    def load_all(self):\n",
    "        print(\"Loading MSMARCO data...\")\n",
    "        with tqdm(total=4, desc=\"Loading components\") as pbar:\n",
    "            self.load_passages()\n",
    "            pbar.update(1)\n",
    "            self.load_passage_embeddings()\n",
    "            pbar.update(1)\n",
    "            self.load_eval_queries()\n",
    "            pbar.update(1)\n",
    "            self.load_dev_queries()\n",
    "            pbar.update(1)\n",
    "    \n",
    "    def load_passages(self):\n",
    "        self.passages = pd.read_csv(\n",
    "            f\"{self.data_dir}/data.tsv\",\n",
    "            sep='\\t', \n",
    "            names=['docid', 'text'],\n",
    "            dtype={'docid': str}\n",
    "        )\n",
    "    \n",
    "    def load_passage_embeddings(self):\n",
    "        try:\n",
    "            ids, embeddings = load_embeddings(\n",
    "                f\"{self.data_dir}/embeddings.h5\",\n",
    "                'id',  # Changed from 'ids' to 'id'\n",
    "                'embedding'  # Changed from 'embeddings' to 'embedding'\n",
    "            )\n",
    "            self.passage_embeddings = dict(zip(ids, embeddings))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading passage embeddings: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_query_embeddings(self) -> Dict[str, np.ndarray]:\n",
    "        ids, embeddings = load_embeddings(\n",
    "            f\"{self.data_dir}/queries_dev_eval_embeddings.h5\",\n",
    "            'id',  # Changed from 'ids' to 'id'\n",
    "            'embedding'  # Changed from 'embeddings' to 'embedding'\n",
    "        )\n",
    "        return dict(zip(ids, embeddings))\n",
    "    \n",
    "    def load_eval_queries(self):\n",
    "        # Load queries and relevance judgments\n",
    "        queries = pd.read_csv(\n",
    "            f\"{self.data_dir}/queries.eval.tsv\",\n",
    "            sep='\\t', \n",
    "            names=['qid', 'query'],\n",
    "            dtype={'qid': str}\n",
    "        )\n",
    "        \n",
    "        qrels = pd.concat([\n",
    "            pd.read_csv(f\"{self.data_dir}/qrels.eval.one.tsv\", \n",
    "                       sep='\\t', names=['qid', '_', 'docid', 'relevance'],\n",
    "                       dtype={'qid': str, 'docid': str}),\n",
    "            pd.read_csv(f\"{self.data_dir}/qrels.eval.two.tsv\", \n",
    "                       sep='\\t', names=['qid', '_', 'docid', 'relevance'],\n",
    "                       dtype={'qid': str, 'docid': str})\n",
    "        ])\n",
    "        \n",
    "        query_embeddings = self.load_query_embeddings()\n",
    "        \n",
    "        self.eval_queries = QuerySet(\n",
    "            qid_to_query={row.qid: row.query for _, row in queries.iterrows()},\n",
    "            qid_to_embedding=query_embeddings,\n",
    "            qid_to_relevance={\n",
    "                qid: {row.docid: row.relevance for _, row in group.iterrows()}\n",
    "                for qid, group in qrels.groupby('qid')\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def load_dev_queries(self):\n",
    "        queries = pd.read_csv(\n",
    "            f\"{self.data_dir}/queries.dev.tsv\",\n",
    "            sep='\\t', \n",
    "            names=['qid', 'query'],\n",
    "            dtype={'qid': str}\n",
    "        )\n",
    "        \n",
    "        qrels = pd.read_csv(\n",
    "            f\"{self.data_dir}/qrels.dev.tsv\",\n",
    "            sep='\\t', \n",
    "            names=['qid', '_', 'docid', 'relevance'],\n",
    "            dtype={'qid': str, 'docid': str}\n",
    "        )\n",
    "        \n",
    "        query_embeddings = self.load_query_embeddings()\n",
    "        \n",
    "        self.dev_queries = QuerySet(\n",
    "            qid_to_query={row.qid: row.query for _, row in queries.iterrows()},\n",
    "            qid_to_embedding=query_embeddings,\n",
    "            qid_to_relevance={\n",
    "                qid: {row.docid: row.relevance for _, row in group.iterrows()}\n",
    "                for qid, group in qrels.groupby('qid')\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def get_passage(self, docid: str) -> str:\n",
    "        try:\n",
    "            return self.passages[self.passages['docid'] == docid]['text'].iloc[0]\n",
    "        except IndexError:\n",
    "            raise KeyError(f\"Passage not found for docid: {docid}\")\n",
    "    \n",
    "    def get_passage_embedding(self, docid: str) -> np.ndarray:\n",
    "        try:\n",
    "            return self.passage_embeddings[docid]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Embedding not found for docid: {docid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MSMARCO data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading components:  25%|██▌       | 1/4 [00:11<00:33, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys in /Users/ad12/Documents/Develop/wse-hw-2/data/embeddings.h5: ['embedding', 'id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading components:  50%|█████     | 2/4 [00:17<00:16,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000000 embeddings from /Users/ad12/Documents/Develop/wse-hw-2/data/embeddings.h5\n",
      "Available keys in /Users/ad12/Documents/Develop/wse-hw-2/data/queries_dev_eval_embeddings.h5: ['embedding', 'id']\n",
      "Loaded 202185 embeddings from /Users/ad12/Documents/Develop/wse-hw-2/data/queries_dev_eval_embeddings.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading components:  75%|███████▌  | 3/4 [00:24<00:07,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys in /Users/ad12/Documents/Develop/wse-hw-2/data/queries_dev_eval_embeddings.h5: ['embedding', 'id']\n",
      "Loaded 202185 embeddings from /Users/ad12/Documents/Develop/wse-hw-2/data/queries_dev_eval_embeddings.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading components: 100%|██████████| 4/4 [00:30<00:00,  7.53s/it]\n"
     ]
    }
   ],
   "source": [
    "data = MSMARCOData(\"/Users/ad12/Documents/Develop/wse-hw-2/data\")\n",
    "data.load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyserini'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyserini\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexWriter\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyserini\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleSearcher\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyserini'"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import faiss\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    docid: str\n",
    "    score: float\n",
    "    rank: int\n",
    "\n",
    "class PurePythonSearchSystem:\n",
    "    def __init__(self, msmarco_data: MSMARCOData):\n",
    "        self.data = msmarco_data\n",
    "        self.bm25 = None\n",
    "        self.docids = None\n",
    "        self.hnsw_index = None\n",
    "        self.tokenized_corpus = None\n",
    "        \n",
    "    def build_bm25_index(self):\n",
    "        \"\"\"Build BM25 index using pure Python implementation\"\"\"\n",
    "        print(\"Building BM25 index...\")\n",
    "        \n",
    "        # Store docids in order\n",
    "        self.docids = self.data.passages['docid'].values\n",
    "        \n",
    "        # Tokenize corpus\n",
    "        self.tokenized_corpus = [\n",
    "            word_tokenize(text.lower())\n",
    "            for text in self.data.passages['text']\n",
    "        ]\n",
    "        \n",
    "        # Create BM25 index\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "        print(\"BM25 index built successfully\")\n",
    "        \n",
    "    def build_hnsw_index(self, m: int = 8, ef_construction: int = 100):\n",
    "        \"\"\"Build HNSW index for vector search\"\"\"\n",
    "        print(\"Building HNSW index...\")\n",
    "        \n",
    "        # Get embeddings in same order as passages\n",
    "        embeddings = np.array([\n",
    "            self.data.passage_embeddings[did] \n",
    "            for did in self.docids\n",
    "        ])\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Initialize and build HNSW index\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.hnsw_index = faiss.IndexHNSWFlat(dimension, m)\n",
    "        self.hnsw_index.hnsw.efConstruction = ef_construction\n",
    "        self.hnsw_index.hnsw.efSearch = ef_construction\n",
    "        \n",
    "        self.hnsw_index.add(embeddings)\n",
    "        print(\"HNSW index built successfully\")\n",
    "        \n",
    "    def bm25_search(self, query: str, k: int = 100) -> List[SearchResult]:\n",
    "        \"\"\"Search using BM25\"\"\"\n",
    "        if self.bm25 is None:\n",
    "            raise RuntimeError(\"BM25 index not built. Call build_bm25_index() first.\")\n",
    "            \n",
    "        # Tokenize query\n",
    "        tokenized_query = word_tokenize(query.lower())\n",
    "        \n",
    "        # Get scores\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Get top k indices\n",
    "        top_indices = np.argsort(-scores)[:k]\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                docid=self.docids[idx],\n",
    "                score=float(scores[idx]),\n",
    "                rank=rank + 1\n",
    "            )\n",
    "            for rank, idx in enumerate(top_indices)\n",
    "        ]\n",
    "        \n",
    "    def vector_search(self, query_embedding: np.ndarray, k: int = 100) -> List[SearchResult]:\n",
    "        \"\"\"Search using HNSW index\"\"\"\n",
    "        if self.hnsw_index is None:\n",
    "            raise RuntimeError(\"HNSW index not built. Call build_hnsw_index() first.\")\n",
    "            \n",
    "        # Normalize query embedding\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Search\n",
    "        distances, indices = self.hnsw_index.search(\n",
    "            query_embedding.reshape(1, -1), k=k\n",
    "        )\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                docid=self.docids[int(idx)],\n",
    "                score=float(1 - dist),\n",
    "                rank=rank + 1\n",
    "            )\n",
    "            for rank, (idx, dist) in enumerate(zip(indices[0], distances[0]))\n",
    "        ]\n",
    "        \n",
    "    def hybrid_search(self, query: str, query_embedding: np.ndarray, \n",
    "                     k: int = 100, alpha: float = 0.5) -> List[SearchResult]:\n",
    "        \"\"\"Hybrid search combining BM25 and dense retrieval\"\"\"\n",
    "        # Get BM25 results\n",
    "        bm25_results = self.bm25_search(query, k=1000)\n",
    "        \n",
    "        # Get embeddings for candidates\n",
    "        candidate_embeddings = np.array([\n",
    "            self.data.get_passage_embedding(res.docid) \n",
    "            for res in bm25_results\n",
    "        ])\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        candidate_embeddings = candidate_embeddings / np.linalg.norm(\n",
    "            candidate_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Compute dense scores\n",
    "        dense_scores = np.dot(candidate_embeddings, query_embedding)\n",
    "        \n",
    "        # Normalize BM25 scores\n",
    "        bm25_scores = np.array([res.score for res in bm25_results])\n",
    "        bm25_scores = (bm25_scores - bm25_scores.min()) / (\n",
    "            bm25_scores.max() - bm25_scores.min()\n",
    "        )\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_scores = alpha * bm25_scores + (1 - alpha) * dense_scores\n",
    "        \n",
    "        # Sort and return top k\n",
    "        top_indices = np.argsort(-combined_scores)[:k]\n",
    "        return [\n",
    "            SearchResult(\n",
    "                docid=bm25_results[idx].docid,\n",
    "                score=float(combined_scores[idx]),\n",
    "                rank=rank + 1\n",
    "            )\n",
    "            for rank, idx in enumerate(top_indices)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_system = PurePythonSearchSystem(data)\n",
    "search_system.build_bm25_index()\n",
    "search_system.build_hnsw_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyserini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
